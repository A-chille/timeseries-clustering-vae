{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vrae.vrae import VRAE\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "plotly.offline.init_notebook_mode()\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "dload = './model_dir_mm' #download directory\n",
    "\n",
    "df = pd.read_pickle(fr'C:\\Users\\achfr\\OneDrive - University of Edinburgh\\Compiled dataset\\df_compiled_mothers_labelled_cyclefate.csv')\n",
    "df_tracks= df.pivot(values=[\n",
    "    'GrowthRateSize','GrowthRateLength','GrowthRateFeretMax','GrowthRateFeretMaxSliding',\n",
    "    'InterdivisionTimes','DivisionRate','DivisionRate_filtered','TrackLength','TrackLength_filtered',\n",
    "    'Size','SizeAtBirthSize','FeretMax','SizeAtBirthFeretMax','MaxLength','SpineLength','SizeAtBirthLength','SpineWidth',\n",
    "    'MeanIntensity_mch','MeanIntensity_gfp','Maxgfp',\n",
    "    'BacteriaLineage','NextDivisionFrame','PreviousDivisionFrame',\n",
    "    'TrackHeadIndices','Prev','Next','Idx','Frame','Indices','PositionIdx','cellcycle_fate'], \n",
    "    index=['Position','ParentTrackHeadIndices','Medium','Treatment','RepeatID','RepeatDate','fate','DeathSubtype'],\n",
    "    columns='Time')\n",
    "\n",
    "exp_name = 'glu_cip_1'\n",
    "\n",
    "medium,treatment,replicate = exp_name.split('_')\n",
    "\n",
    "skip_timepoints = 1\n",
    "if medium == 'gly': skip_timepoints = 2  #in glycerol data is missing every other timepoint because data was collected every 10 minutes instead of 5 so we need to skip every other timepoint\n",
    "\n",
    "frame = df_tracks.loc(axis=0)[:,:,medium,treatment,replicate]\n",
    "size_array = np.array([list(frame['FeretMax'].T[k]) for k in frame['FeretMax'].T.keys()])\n",
    "size_array = TimeSeriesScalerMeanVariance().fit_transform(size_array)\n",
    "cyclefate_array = np.array([list(frame['cellcycle_fate'].T[k]) for k in frame['cellcycle_fate'].T.keys()])\n",
    "fates = np.array([i for i in frame.reset_index('fate')['fate']])\n",
    "death_subtypes = np.array([i for i in frame.reset_index('DeathSubtype')['DeathSubtype']])\n",
    "\n",
    "\n",
    "liste=[]\n",
    "for i in range(size_array.shape[0]):\n",
    "    serie =  size_array[i]\n",
    "    t_death = np.where(cyclefate_array[i,range(0,cyclefate_array.shape[1],skip_timepoints)] != 'alive')[0][0]\n",
    "    usable_data = serie[24:min(t_death,168)]\n",
    "    seq_len = 70\n",
    "    # for i in range(len(usable_data)//seq_len):  #splitting the data into sequences of length seq_len\n",
    "    #     tre = (i+1)*seq_len\n",
    "    #     liste.append(usable_data[tre-seq_len:tre])\n",
    "    for i in range(0,len(usable_data)-seq_len,1):\n",
    "        liste.append(usable_data[i:i+seq_len])\n",
    "        \n",
    "Data = np.array(np.random.permutation(liste))[:,:,:]\n",
    "Data.shape\n",
    "\n",
    "X_train, X_test = Data[:int(0.8*Data.shape[0])], Data[int(0.8*Data.shape[0]):]\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test))\n",
    "\n",
    "sequence_length = X_train.shape[1]\n",
    "number_of_features = X_train.shape[2]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 20\n",
    "batch_size = 30\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 100\n",
    "dropout_rate = 0.2\n",
    "optimizer = 'Adam' # options: ADAM, SGD\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "loss = 'MSELoss' # options: SmoothL1Loss, MSELoss\n",
    "block = 'LSTM' # options: LSTM, GRU\n",
    "\n",
    "vrae = VRAE(sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            optimizer = optimizer, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            loss = loss,\n",
    "            block = block,\n",
    "            dload = dload)\n",
    "\n",
    "vrae.fit(train_dataset, test_dataset)\n",
    "\n",
    "vrae.eval()\n",
    "testseq = test_dataset[30:batch_size+30][0].float()\n",
    "print(testseq.shape)\n",
    "# plt.plot(testseq[:,:,0].T)\n",
    "testseq2 = testseq.permute(1, 0, 2).cuda()\n",
    "print(testseq2.shape)\n",
    "outp = vrae.forward(testseq2)\n",
    "print(outp[0].shape)\n",
    "\n",
    "k=np.random.randint(0,batch_size)\n",
    "input_seq = testseq2[:,k,0].cpu().detach().numpy()\n",
    "output_seq = outp[0][:,k,0].cpu().detach().numpy()\n",
    "plt.figure()\n",
    "plt.plot(input_seq.T)\n",
    "plt.plot(output_seq.T)\n",
    "import torch.nn as nn   \n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "loss(testseq2, outp[0])\n",
    "\n",
    "# 6min"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
